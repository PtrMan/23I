{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "138c840d-bdc8-42bd-87b5-8a9d76f33cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /usr/local/lib/python3.9/dist-packages (2.9.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60dbe125-f3b2-4e99-b778-f2aab26ed042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64368b68-f0ff-422f-b34b-416f1779206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, GRU, Embedding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9d1d917-bf53-4cdf-b05e-4e77a755b384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 25204\n"
     ]
    }
   ],
   "source": [
    "data_text = \"\"\n",
    "\n",
    "\n",
    "for iPath in ['trainSmallQa.txt', 'trainSmallWorld0.txt']:#, 'trainSmallMath0.txt', 'trainEnToNarsese0.txt', 'trainMisc0.txt', 'trainSmallPhysics0.txt', 'trainFullWiki0.txt', 'trainMisc1.txt']:\n",
    "    with open(iPath, 'r') as file:\n",
    "        content = file.read()\n",
    "        data_text = data_text+content\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def text_cleaner(text):\n",
    "    # lower case text\n",
    "    text2 = text.lower()\n",
    "    #text2 = re.sub(r\"'s\\b\",\"\", text2)\n",
    "    # remove punctuations\n",
    "    #text2 = re.sub(\"[^a-zA-Z]\", \" \", text2) \n",
    "    #print(text2) # DBG\n",
    "    \n",
    "    long_words=[]\n",
    "    #\n",
    "    # remove short word\n",
    "    for i in text2.split():\n",
    "        if len(i)>=2:                  \n",
    "            long_words.append(i)\n",
    "    return (\" \".join(long_words)).strip()\n",
    "    #return text2\n",
    "\n",
    "# preprocess the text\n",
    "data_new = text_cleaner(data_text)\n",
    "\n",
    "\n",
    "# store cleaned text for debugging\n",
    "if True: # code block\n",
    "    f = open('z_db_preprocessedDump.txt', 'w')\n",
    "    f.write(data_new)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_seq(text):\n",
    "    length = 30\n",
    "    sequences = list()\n",
    "    for i in range(length, len(text)):\n",
    "        # select sequence of tokens\n",
    "        seq = text[i-length:i+1]\n",
    "        # store\n",
    "        sequences.append(seq)\n",
    "    print('Total Sequences: %d' % len(sequences))\n",
    "    return sequences\n",
    "\n",
    "# create sequences   \n",
    "sequences = create_seq(data_new)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create a character mapping index\n",
    "chars = sorted(list(set(data_new)))\n",
    "mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "def encode_seq(seq):\n",
    "    sequences = list()\n",
    "    for line in seq:\n",
    "        # integer encode line\n",
    "        encoded_seq = [mapping[char] for char in line]\n",
    "        # store\n",
    "        sequences.append(encoded_seq)\n",
    "    return sequences\n",
    "\n",
    "# encode the sequences\n",
    "sequences = encode_seq(sequences)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# predict\n",
    "\n",
    "import numpy as np\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of characters\n",
    "    for _ in range(n_chars):\n",
    "        # encode the characters as integers\n",
    "        encoded = [mapping[char] for char in in_text]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict character probabilities\n",
    "        yhat = model.predict(encoded, verbose=0)\n",
    "        # get the index of the highest probability character\n",
    "        yhat = np.argmax(yhat)\n",
    "        # reverse map integer to character\n",
    "        out_char = ''\n",
    "        for char, index in mapping.items():\n",
    "            if index == yhat:\n",
    "                out_char = char\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += char\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38eab7cc-7e2d-45e9-8356-d1e5c7f68723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (23691, 30) Val shape: (1513, 30)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# vocabulary size\n",
    "vocab = len(mapping)\n",
    "sequences = np.array(sequences)\n",
    "# create X and y\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "# one hot encode y\n",
    "y = to_categorical(y, num_classes=vocab)\n",
    "# create train and validation sets\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.06, random_state=42)\n",
    "\n",
    "print('Train shape:', X_tr.shape, 'Val shape:', X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94b3c3b4-d5e0-42b1-a023-729e96f0aa69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 30, 70)            5110      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 30, 325)           23075     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 140)               196140    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 73)                10293     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 234,618\n",
      "Trainable params: 234,618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab, 70, input_length=30, trainable=True)) # 50\n",
    "#model.add(Dense(440, activation='relu')) # 0     # 50\n",
    "#model.add(GRU(400, return_sequences=False, recurrent_dropout=0.1, dropout=0.1)) # 200    # 185 # 170 # 150\n",
    "\n",
    "s0 = 1.0 # 70'000parameters\n",
    "s0 = 1.5 # 70'000parameters\n",
    "#s0 = 1.5*3.0*1.2\n",
    "s0 = 0.8 # 47'000 parameters for 7800 samples\n",
    "s0 = 1.5\n",
    "s0 = 1.75\n",
    "model.add(Dense(int(124*s0*1.5), activation='relu')) # 0     # 50\n",
    "model.add(GRU(int(80*s0), return_sequences=False, recurrent_dropout=0.1, dropout=0.1)) # 200    # 185 # 170 # 150\n",
    "\n",
    "\n",
    "model.add(Dense(vocab, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "# compile the model\n",
    "learning_rate = 0.0008 # 0.0005\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer=optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de27d22-3fd5-439f-814f-592bf2ecd3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    import keras\n",
    "    \n",
    "    print('loading...')\n",
    "    model = keras.models.load_model('lm87A.keras.model')\n",
    "    print('loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29196c5b-9817-40cf-913f-ebf3b58be05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module which is used to decide when the search for a better network is aborted based on last epoch of improvement in test-set\n",
    "class CountDown(object):\n",
    "    def __init__(self, countdownVal):\n",
    "        self.countdownVal = countdownVal\n",
    "        self.v = -self.countdownVal # counter\n",
    "    \n",
    "    def reset(self):\n",
    "        self.v = -self.countdownVal\n",
    "    \n",
    "    def count(self):\n",
    "        self.v += 1\n",
    "    \n",
    "    def check(self):\n",
    "        return self.v >= 0\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea86b28-3a63-494e-8d86-7015e3b4c523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch=0\n",
      "741/741 - 60s - loss: 2.5178 - acc: 0.2884 - val_loss: 2.2153 - val_acc: 0.3437 - 60s/epoch - 81ms/sample\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 2.21529, saving model to lm87A_lowestValidationLoss.keras.model\n",
      "INFO:tensorflow:Assets written to: lm87A_lowestValidationLoss.keras.model/assets\n",
      "INFO:tensorflow:Assets written to: lm87A.keras.model/assets\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 2.2153 - acc: 0.3437\n",
      "Validation loss=2.2153, Validation accuracy=0.3437\n",
      "sample=exponentiation operation is and and and and and and and and and and and and and and a\n",
      "sample=sine and cosine are trigonometric functions angle and and and and and and and and and and and and and and and and and and and and\n",
      "sample=the oldest definitions trigonometric functions and and and and and and and and and and and and and and and and and and and and\n",
      "sample=real number number that cand is and and and and and and and and and and and and and and and and and and and\n",
      "sample=extract relations and and and and and and and and and and and and and and and and and and and and\n",
      "sample=water is and and and and and and and and and and and and and and and and and and and \n",
      "sample=q: what is food? a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a:\n",
      "sample=q: what is llama? a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a:\n",
      "sample=q: what is google? a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a:\n",
      "sample=q: what is frog? a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a:\n",
      "sample=q: what is llgic? a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a: a:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch=1\n",
      "741/741 - 57s - loss: 2.0887 - acc: 0.3857 - val_loss: 2.0374 - val_acc: 0.4005 - 57s/epoch - 77ms/sample\n",
      "\n",
      "Epoch 1: val_loss improved from 2.21529 to 2.03742, saving model to lm87A_lowestValidationLoss.keras.model\n",
      "INFO:tensorflow:Assets written to: lm87A_lowestValidationLoss.keras.model/assets\n",
      "INFO:tensorflow:Assets written to: lm87A.keras.model/assets\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 2.0374 - acc: 0.4005\n",
      "Validation loss=2.0374, Validation accuracy=0.4005\n",
      "sample=exponentiation operation of the of the of the of the of the of the of the of the of t\n",
      "sample=sine and cosine are trigonometric functions angle is ... the of the of the of the of the of the of the of the of the of the of th\n",
      "sample=the oldest definitions trigonometric functions of the of the of the of the of the of the of the of the of the of the of the of\n",
      "sample=real number number that cand is ... the of the of the of the of the of the of the of the of the of the of t\n",
      "sample=extract relations of the of the of the of the of the of the of the of the of the of the of the of\n",
      "sample=water is ... the of the of the of the of the of the of the of the of the of the of th\n",
      "sample=q: what is food? a: the of the of the of the of the of the of the of the of the of the of the of \n",
      "sample=q: what is llama? a: the of the of the of the of the of the of the of the of the of the of the of \n",
      "sample=q: what is google? a: the of the of the of the of the of the of the of the of the of the of the of \n",
      "sample=q: what is frog? a: the of the of the of the of the of the of the of the of the of the of the of \n",
      "sample=q: what is llgic? a: the of the of the of the of the of the of the of the of the of the of the of \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch=2\n",
      "741/741 - 56s - loss: 1.9099 - acc: 0.4370 - val_loss: 1.9258 - val_acc: 0.4461 - 56s/epoch - 75ms/sample\n",
      "\n",
      "Epoch 1: val_loss improved from 2.03742 to 1.92576, saving model to lm87A_lowestValidationLoss.keras.model\n",
      "INFO:tensorflow:Assets written to: lm87A_lowestValidationLoss.keras.model/assets\n",
      "INFO:tensorflow:Assets written to: lm87A.keras.model/assets\n",
      "48/48 [==============================] - 1s 11ms/step - loss: 1.9258 - acc: 0.4461\n",
      "Validation loss=1.9258, Validation accuracy=0.4461\n",
      "sample=exponentiation operation of the sear suble the sear suble the sear suble the sear sub\n",
      "sample=sine and cosine are trigonometric functions angle the sear suble the sear suble the sear suble the sear suble the sear suble the \n",
      "sample=the oldest definitions trigonometric functions of the sear suble the sear suble the sear suble the sear suble the sear suble t\n",
      "sample=real number number that cance of the sear suble the sear suble the sear suble the sear suble the sear suble\n",
      "sample=extract relations of the sear suble the sear suble the sear suble the sear suble the sear suble t\n",
      "sample=water is the sear suble the sear suble the sear suble the sear suble the sear suble t\n",
      "sample=q: what is food? a: what is the sear suble the sear suble the sear suble the sear suble the sear \n",
      "sample=q: what is llama? a: what is the sear suble the sear suble the sear suble the sear suble the sear \n",
      "sample=q: what is google? a: what is the sear suble the sear suble the sear suble the sear suble the sear \n",
      "sample=q: what is frog? a: what is the sear suble the sear suble the sear suble the sear suble the sear \n",
      "sample=q: what is llgic? a: what is the sear suble the sear suble the sear suble the sear suble the sear \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch=3\n",
      "741/741 - 56s - loss: 1.7832 - acc: 0.4713 - val_loss: 1.8464 - val_acc: 0.4732 - 56s/epoch - 76ms/sample\n",
      "\n",
      "Epoch 1: val_loss improved from 1.92576 to 1.84637, saving model to lm87A_lowestValidationLoss.keras.model\n",
      "INFO:tensorflow:Assets written to: lm87A_lowestValidationLoss.keras.model/assets\n",
      "INFO:tensorflow:Assets written to: lm87A.keras.model/assets\n",
      "48/48 [==============================] - 1s 10ms/step - loss: 1.8464 - acc: 0.4732\n",
      "Validation loss=1.8464, Validation accuracy=0.4732\n",
      "sample=exponentiation operation of the or and of the or and of the or and of the or and of t\n",
      "sample=sine and cosine are trigonometric functions angle of the or and of the or and of the or and of the or and of the or and of the or\n",
      "sample=the oldest definitions trigonometric functions of the or and of the or and of the or and of the or and of the or and of the or\n",
      "sample=real number number that can of the or and of the or and of the or and of the or and of the or and of the or\n",
      "sample=extract relations of the or and of the or and of the or and of the or and of the or and of the or\n",
      "sample=water is ... an of the or and of the or and of the or and of the or and of the or and\n",
      "sample=q: what is food? a: q: what is ... an of the or and of the or and of the or and of the or and of \n",
      "sample=q: what is llama? a: q: what is ... an of the or and of the or and of the or and of the or and of \n",
      "sample=q: what is google? a: q: what is ... an of the or and of the or and of the or and of the or and of \n",
      "sample=q: what is frog? a: q: what is ... an of the or and of the or and of the or and of the or and of \n",
      "sample=q: what is llgic? a: q: what is ... an of the or and of the or and of the or and of the or and of \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch=4\n",
      "741/741 - 56s - loss: 1.6875 - acc: 0.5016 - val_loss: 1.7956 - val_acc: 0.4865 - 56s/epoch - 76ms/sample\n",
      "\n",
      "Epoch 1: val_loss improved from 1.84637 to 1.79563, saving model to lm87A_lowestValidationLoss.keras.model\n",
      "INFO:tensorflow:Assets written to: lm87A_lowestValidationLoss.keras.model/assets\n",
      "INFO:tensorflow:Assets written to: lm87A.keras.model/assets\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 1.7956 - acc: 0.4865\n",
      "Validation loss=1.7956, Validation accuracy=0.4865\n",
      "sample=exponentiation operation of the sestem and the sestem and the sestem and the sestem a\n",
      "sample=sine and cosine are trigonometric functions angless and the sestem and the sestem and the sestem and the sestem and the sestem an\n",
      "sample=the oldest definitions trigonometric functions of the seation of the seation of the seation of the seation of the seation of t\n",
      "sample=real number number that can some the object is ... an object is ... an object is ... an object is ... an ob\n",
      "sample=extract relations and the sestem and the sestem and the sestem and the sestem and the sestem and \n",
      "sample=water is ... an object is ... an object is ... an object is ... an object is ... an o\n",
      "sample=q: what is food? a: an of the sestem and the sestem and the sestem and the sestem and the sestem \n",
      "sample=q: what is llama? a: an of the sestem and the sestem and the sestem and the sestem and the sestem \n",
      "sample=q: what is google? a: an of the sestem and the sestem and the sestem and the sestem and the sestem \n",
      "sample=q: what is frog? a: an of the sestem and the sestem and the sestem and the sestem and the sestem \n",
      "sample=q: what is llgic? a: an of the sestem and the sestem and the sestem and the sestem and the sestem \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch=5\n",
      "741/741 - 56s - loss: 1.6070 - acc: 0.5260 - val_loss: 1.7605 - val_acc: 0.4950 - 56s/epoch - 75ms/sample\n",
      "\n",
      "Epoch 1: val_loss improved from 1.79563 to 1.76045, saving model to lm87A_lowestValidationLoss.keras.model\n",
      "INFO:tensorflow:Assets written to: lm87A_lowestValidationLoss.keras.model/assets\n",
      "INFO:tensorflow:Assets written to: lm87A.keras.model/assets\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 1.7605 - acc: 0.4950\n",
      "Validation loss=1.7605, Validation accuracy=0.4950\n",
      "sample=exponentiation operation of the space the space the space the space the space the spa\n",
      "sample=sine and cosine are trigonometric functions angles of the space the space the space the space the space the space the space the s\n",
      "sample=the oldest definitions trigonometric functions of the space the space the space the space the space the space the space the sp\n",
      "sample=real number number that can species of the space the space the space the space the space the space the spac\n",
      "sample=extract relations and on object of the space the space the space the space the space the space th\n",
      "sample=water is ... and on object of the space the space the space the space the space the s\n",
      "sample=q: what is food? a: mat is the space the space the space the space the space the space the space \n",
      "sample=q: what is llama? a: mat is the space the space the space the space the space the space the space \n",
      "sample=q: what is google? a: mat is the space the space the space the space the space the space the space \n",
      "sample=q: what is frog? a: mat is the space the space the space the space the space the space the space \n",
      "sample=q: what is llgic? a: mat is the space the space the space the space the space the space the space \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch=6\n",
      "741/741 - 56s - loss: 1.5490 - acc: 0.5394 - val_loss: 1.7213 - val_acc: 0.5129 - 56s/epoch - 76ms/sample\n",
      "\n",
      "Epoch 1: val_loss improved from 1.76045 to 1.72134, saving model to lm87A_lowestValidationLoss.keras.model\n",
      "INFO:tensorflow:Assets written to: lm87A_lowestValidationLoss.keras.model/assets\n",
      "INFO:tensorflow:Assets written to: lm87A.keras.model/assets\n",
      "48/48 [==============================] - 1s 10ms/step - loss: 1.7213 - acc: 0.5129\n",
      "Validation loss=1.7213, Validation accuracy=0.5129\n",
      "sample=exponentiation operation of the simples of the semal species of the semal species of \n",
      "sample=sine and cosine are trigonometric functions angless and in the simples of the semal species of the semal species of the semal spe\n",
      "sample=the oldest definitions trigonometric functions of the semal species of the semal species of the semal species of the semal spe\n",
      "sample=real number number that can some the world and in the simples of the semal species of the semal species of \n",
      "sample=extract relations and the sempless and in the simples of the semal species of the semal species o\n",
      "sample=water is ... an objection of the simples of the semal species of the semal species of\n",
      "sample=q: what is food? a: mot is ... an objection of the simples of the semal species of the semal spec\n",
      "sample=q: what is llama? a: mot is ... an objection of the simples of the semal species of the semal spec\n",
      "sample=q: what is google? a: mot is ... an objection of the simples of the semal species of the semal spec\n",
      "sample=q: what is frog? a: mot is ... an objection of the simples of the semal species of the semal spec\n",
      "sample=q: what is llgic? a: mot is ... an objection of the simples of the semal species of the semal spec\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch=7\n",
      "741/741 - 56s - loss: 1.4948 - acc: 0.5550 - val_loss: 1.7204 - val_acc: 0.5096 - 56s/epoch - 76ms/sample\n",
      "\n",
      "Epoch 1: val_loss improved from 1.72134 to 1.72044, saving model to lm87A_lowestValidationLoss.keras.model\n",
      "INFO:tensorflow:Assets written to: lm87A_lowestValidationLoss.keras.model/assets\n",
      "INFO:tensorflow:Assets written to: lm87A.keras.model/assets\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 1.7204 - acc: 0.5096\n",
      "Validation loss=1.7204, Validation accuracy=0.5096\n",
      "sample=exponentiation operation of the order of the order of the order of the order of the o\n",
      "sample=sine and cosine are trigonometric functions angle of the order of the order of the order of the order of the order of the order o\n",
      "sample=the oldest definitions trigonometric functions of the order of the order of the order of the order of the order of the order o\n",
      "sample=real number number that can sustance of the order of the order of the order of the order of the order of th\n",
      "sample=extract relations of the order of the order of the order of the order of the order of the order o\n",
      "sample=water is ... are the only of the order of the order of the order of the order of the \n",
      "sample=q: what is food? a: art of the order of the order of the order of the order of the order of the o\n",
      "sample=q: what is llama? a: an object is ... are the only of the order of the order of the order of the o\n",
      "sample=q: what is google? a: an object is ... are the only of the order of the order of the order of the o\n",
      "sample=q: what is frog? a: an object is ... are the only of the order of the order of the order of the o\n",
      "sample=q: what is llgic? a: an object is ... are the only of the order of the order of the order of the o\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "epoch=8\n"
     ]
    }
   ],
   "source": [
    "# Define a checkpoint to save the model with the lowest validation loss\n",
    "checkpoint = ModelCheckpoint('lm87A_lowestValidationLoss.keras.model', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "abortCountdown = CountDown(130) # give up after 13 of epochs of no improvement\n",
    "bestValidationsetLoss = 1e10\n",
    "\n",
    "# fit the model\n",
    "for it in range(int(250/1)):\n",
    "    for z in range(4):\n",
    "        print('')\n",
    "    \n",
    "    \n",
    "    print(f\"epoch={it*1}\")\n",
    "    \n",
    "    model.fit(X_tr, y_tr, epochs=1, verbose=2, validation_data=(X_val, y_val), callbacks=[keras.callbacks.ProgbarLogger(), checkpoint])\n",
    "    model.save('lm87A.keras.model')\n",
    "    \n",
    "    \n",
    "    # Evaluate the model on the validation set and print the loss and accuracy\n",
    "    loss, acc = model.evaluate(X_val, y_val)\n",
    "    print(f'Validation loss={loss:.4f}, Validation accuracy={acc:.4f}')\n",
    "    \n",
    "    \n",
    "    if True: # codeblock\n",
    "        abortCountdown.count()\n",
    "        if loss < bestValidationsetLoss:\n",
    "            bestValidationsetLoss = loss\n",
    "            abortCountdown.reset() # reset counter because it's better\n",
    "\n",
    "        if abortCountdown.check(): # is countdown fullfilled\n",
    "            # this means that the countdown is over, and this means that we didn't find a better model for a long time\n",
    "            print('info: didn\\'t find a better model for a to long time ... give up')\n",
    "            break\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #s0 = generate_seq(model, mapping, 30, \"q: what is a llama?\", 60)\n",
    "    #print(f'sample={s0}')\n",
    "    \n",
    "    #s0 = generate_seq(model, mapping, 30, \"q: what is a neutron?\", 60)\n",
    "    #print(f'sample={s0}')\n",
    "    \n",
    "    s0 = generate_seq(model, mapping, 30, \"exponentiation operation \", 60)\n",
    "    print(f'sample={s0}')\n",
    "    \n",
    "    s0 = generate_seq(model, mapping, 30, \"sine and cosine are trigonometric functions angle\", 80)\n",
    "    print(f'sample={s0}')\n",
    "\n",
    "    s0 = generate_seq(model, mapping, 30, \"the oldest definitions trigonometric functions\", 80)\n",
    "    print(f'sample={s0}')\n",
    "\n",
    "    \n",
    "    s0 = generate_seq(model, mapping, 30, \"real number number that can\", 80)\n",
    "    print(f'sample={s0}')\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    s0 = generate_seq(model, mapping, 30, \"extract relations\", 80)\n",
    "    print(f'sample={s0}')\n",
    "        \n",
    "    s0 = generate_seq(model, mapping, 30, \"water\", 80)\n",
    "    print(f'sample={s0}')\n",
    "    \n",
    "    for iQueryInner in ['food', 'llama', 'google', 'frog', 'llgic']:\n",
    "        s0 = generate_seq(model, mapping, 30, f\"q: what is {iQueryInner}? \", 80)\n",
    "        print(f'sample={s0}')\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a73eaa86-aaa6-4b70-8c28-516075daf5fb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "s0 = generate_seq(model, mapping, 30, \"the english word world comes from\", 80)\n",
    "print(f'sample={s0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a01aeb-33e4-4268-bbbb-649538160482",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "930fb4bd-709f-4169-9f77-18b9aa84d52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
