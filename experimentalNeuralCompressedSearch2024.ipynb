{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b195c3d-9cba-4b7f-8cf3-5296f5e8f7f9",
   "metadata": {},
   "source": [
    "**experimental implementation of neural compressed search inspired learning algorithm (Schmidhuber)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "421745a6-26ad-4a3a-a1a1-b8da93e89569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "6a4aa8ec-3c15-4715-b4ab-4042f9930f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose2(x):\n",
    "   return torch.transpose(x, 0, 1)\n",
    "\n",
    "\n",
    "# \"Hopfield\"\n",
    "class Hopfield(torch.nn.Module):\n",
    "    def __init__(self, n, width, beta=0.1):\n",
    "        super(Hopfield, self).__init__()\n",
    "        self.beta = beta\n",
    "\n",
    "        self.n = n # number of items\n",
    "        self.width = width # width of a memory item\n",
    "\n",
    "\n",
    "        self.Wk = torch.nn.Parameter(torch.rand(self.width, self.n)*(1.0 / (self.width*self.n))*1.0) # key\n",
    "        self.Wv = torch.nn.Parameter(torch.rand(self.width, self.n)*(1.0 / (self.width*self.n))*0.1) # value\n",
    "        self.Wq = torch.nn.Parameter(torch.rand(self.n, self.n)*(1.0 / (self.n*self.n))*0.1) # query (is always a square matrix)\n",
    "\n",
    "    # /param R retrieval\n",
    "    def forward(self, R, Y):\n",
    "        z0 = self.beta * R\n",
    "        z0 = z0 @ self.Wq\n",
    "        z0 = z0 @ transpose2(self.Wk)\n",
    "        z0 = z0 @ transpose2(Y)\n",
    "        z0 = torch.nn.functional.softmax(z0, dim=-1)  # Apply softmax\n",
    "\n",
    "        z0 = z0 @ Y\n",
    "        z = z0 @ self.Wv\n",
    "        return z\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LeakyReLUMlp(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, negative_slope=0.05):\n",
    "        super(LeakyReLUMlp, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.leaky_relu = torch.nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "90067832-4663-456c-86df-e10eb869e07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0081950873, 0.0066402457, 0.0053359563],\n",
      "        [0.0081950650, 0.0066402261, 0.0053359480],\n",
      "        [0.0081951059, 0.0066402676, 0.0053359703],\n",
      "        [0.0081950705, 0.0066402294, 0.0053359489],\n",
      "        [0.0081950678, 0.0066402252, 0.0053359456],\n",
      "        [0.0081950482, 0.0066402042, 0.0053359340],\n",
      "        [0.0081950482, 0.0066402047, 0.0053359340],\n",
      "        [0.0081950482, 0.0066402047, 0.0053359340],\n",
      "        [0.0081950482, 0.0066402042, 0.0053359340]], grad_fn=<MmBackward0>)\n",
      "torch.Size([27])\n",
      "torch.Size([27, 4])\n",
      "tensor([0.1008, 0.0937, 0.0817, 0.0995], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "hopfield = Hopfield(3, 9)\n",
    "\n",
    "R = transpose2( torch.tensor([[0.99, 0.01, 0.998,  0.11, 0.01, 0.00282,  0.00827, 0.0082629, 0.000986], [0.00882882, 0.992, 0.998,  0.11, 0.01, 0.00282,  0.00827, 0.0082629, 0.000986], [0.99, 0.01, 0.998,  0.999, 0.999, 0.00282,  0.00827, 0.0082629, 0.000986]]) )\n",
    "Y = torch.rand(hopfield.n, hopfield.width)\n",
    "hopfieldOutA = hopfield.forward(R, Y)\n",
    "\n",
    "\n",
    "torch.set_printoptions(precision=10)\n",
    "print(hopfieldOutA)\n",
    "torch.set_printoptions(profile='default')\n",
    "\n",
    "m0 = torch.rand(hopfield.n*hopfield.width, 4)\n",
    "\n",
    "m1 = torch.flatten(hopfieldOutA)\n",
    "\n",
    "print(m1.size())\n",
    "print(m0.size())\n",
    "\n",
    "z0 = m1 @ m0\n",
    "\n",
    "modelOut = z0\n",
    "\n",
    "print(z0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "2bfdbab6-4874-40f0-a8dd-c7da8679cb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "torch.Size([9, 3])\n",
      "Wk   Parameter containing:\n",
      "tensor([[0.0204, 0.0349, 0.0202],\n",
      "        [0.0302, 0.0190, 0.0328],\n",
      "        [0.0261, 0.0050, 0.0228],\n",
      "        [0.0041, 0.0306, 0.0221],\n",
      "        [0.0282, 0.0172, 0.0218],\n",
      "        [0.0296, 0.0333, 0.0114],\n",
      "        [0.0259, 0.0132, 0.0360],\n",
      "        [0.0300, 0.0281, 0.0205],\n",
      "        [0.0247, 0.0164, 0.0328]], requires_grad=True)\n",
      "torch.Size([9, 3])\n",
      "Wv   Parameter containing:\n",
      "tensor([[0.0020, 0.0001, 0.0005],\n",
      "        [0.0027, 0.0011, 0.0003],\n",
      "        [0.0036, 0.0021, 0.0027],\n",
      "        [0.0033, 0.0019, 0.0028],\n",
      "        [0.0026, 0.0007, 0.0008],\n",
      "        [0.0015, 0.0022, 0.0036],\n",
      "        [0.0028, 0.0008, 0.0003],\n",
      "        [0.0008, 0.0037, 0.0015],\n",
      "        [0.0008, 0.0037, 0.0011]], requires_grad=True)\n",
      "torch.Size([3, 3])\n",
      "Wq   Parameter containing:\n",
      "tensor([[0.0087, 0.0036, 0.0054],\n",
      "        [0.0011, 0.0110, 0.0031],\n",
      "        [0.0055, 0.0048, 0.0064]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "print('\\n'*3)\n",
    "\n",
    "for iParamName, iParam in hopfield.named_parameters():\n",
    "    paramTensor = iParam.data # access actual tensor of parameter\n",
    "    print(paramTensor.size()) # access actual tensor\n",
    "    \n",
    "    print(iParamName + '   ' + str(iParam))\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    if iParamName == 'Wq':\n",
    "        # we treat Wq in a special way\n",
    "        \n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "60ca09a1-13b6-4612-8fbf-02bd9ee1d61d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nHyperParams=11328\n",
      "hypersBestLoss=0.32218170166015625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[299], line 339\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03mparams1 = readoutCtx.readoutMatrixFromCompressed(hypers, nUnits, matrixWidth, ncsCtx) # translate hyper-parameters to actual parameters as a matrix\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m#print(params1.size()) # DBG\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# write parameters from hyperparameters to actual model\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m \u001b[43mreadoutParamsOfModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhypers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mncsCtx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m####################\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# INFERENCE\u001b[39;00m\n\u001b[1;32m    347\u001b[0m yTarget \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.2\u001b[39m])\n",
      "Cell \u001b[0;32mIn[299], line 257\u001b[0m, in \u001b[0;36mreadoutParamsOfModel\u001b[0;34m(hypers, isWrite, model, ncsCtx)\u001b[0m\n\u001b[1;32m    254\u001b[0m     matrixWidth5 \u001b[38;5;241m=\u001b[39m paramTensor\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    255\u001b[0m     nUnits5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 257\u001b[0m z0 \u001b[38;5;241m=\u001b[39m \u001b[43mreadoutCtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadoutMatrixFromCompressed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhypers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnUnits5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatrixWidth5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mncsCtx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isWrite:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m#print('---')\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;66;03m#print(iParam.data.size())\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     z0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtranspose(z0, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[299], line 74\u001b[0m, in \u001b[0;36mReadoutCtx.readoutMatrixFromCompressed\u001b[0;34m(self, hypers, nUnits, matrixWidth, ncsCtx)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypersIdx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (nUnits\u001b[38;5;241m*\u001b[39mncsCtx\u001b[38;5;241m.\u001b[39mnWaves\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoOutMode:\n\u001b[0;32m---> 74\u001b[0m         params0 \u001b[38;5;241m=\u001b[39m \u001b[43mncsCtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalcSuperpositionByHyperparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatrixWidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypersSlice0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m         paramStack\u001b[38;5;241m.\u001b[39mappend(params0)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoOutMode:\n",
      "Cell \u001b[0;32mIn[299], line 25\u001b[0m, in \u001b[0;36mNeuralCompressedSearchCtx.calcSuperpositionByHyperparameters\u001b[0;34m(self, n, hyperParameters)\u001b[0m\n\u001b[1;32m     22\u001b[0m params \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(n)\n\u001b[1;32m     24\u001b[0m curIdx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m curIdx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(hyperParameters):\n\u001b[1;32m     26\u001b[0m     freq \u001b[38;5;241m=\u001b[39m hyperParameters[curIdx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m0\u001b[39m]        \n\u001b[1;32m     27\u001b[0m     phase \u001b[38;5;241m=\u001b[39m hyperParameters[curIdx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/torch/_tensor.py:1001\u001b[0m, in \u001b[0;36mTensor.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlen() of a 0-d tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tracing_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1002\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1003\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing len to get tensor shape might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1004\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecommended usage would be tensor.shape[0]. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1008\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1009\u001b[0m     )\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# context to keep parameters etc of \"neural compressed search\" learning algorithm from Schmidhuber\n",
    "class NeuralCompressedSearchCtx(object):\n",
    "    def __init__(self):\n",
    "\n",
    "        #self.n = 10 # size of parameter vector to generate\n",
    "\n",
    "        #self.hypersIdx = 0 # current index into hyperparameters for read out to actual parameters\n",
    "\n",
    "        self.nWaves = 2\n",
    "\n",
    "        pass\n",
    "\n",
    "    # compute the superposition of weights for the parameterization of the NN by a given hyperparameter vector\n",
    "    def calcSuperpositionByHyperparameters(self, n, hyperParameters):\n",
    "        if len(hyperParameters) % 3 != 0:\n",
    "            raise Exception('invalid length of hyperparameters!')\n",
    "        \n",
    "        #print(hyperParameters)\n",
    "        #print(n)\n",
    "        \n",
    "        \n",
    "        params = torch.zeros(n)\n",
    "        \n",
    "        curIdx=0\n",
    "        while curIdx < len(hyperParameters):\n",
    "            freq = hyperParameters[curIdx+0]        \n",
    "            phase = hyperParameters[curIdx+1]\n",
    "            amplitude = hyperParameters[curIdx+2]\n",
    "            curIdx+=3\n",
    "            \n",
    "            v0 = torch.arange(0,n, dtype=torch.float32) # generate increasing tensor\n",
    "            \n",
    "            v1 = v0*freq + torch.ones(n)*phase\n",
    "            \n",
    "            # now we compute the parameters\n",
    "            params0 = torch.cos(v1)*amplitude\n",
    "\n",
    "            params += params0 # add wave to get a superposition of the waves\n",
    "\n",
    "        return params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# context for readout of parameters from hyperparameters\n",
    "class ReadoutCtx(object):\n",
    "    def __init__(self):\n",
    "        self.hypersIdx = 0 # current index into hyperparameters for read out to actual parameters\n",
    "\n",
    "\n",
    "        # defines if a output is returned (this is useful to compute number of hyperparameters based on the actual indices)\n",
    "        self.noOutMode = False\n",
    "    \n",
    "    # read-out\n",
    "    def readoutMatrixFromCompressed(self, hypers, nUnits, matrixWidth, ncsCtx):\n",
    "        #print(f'DBG hypersIdx={self.hypersIdx}')\n",
    "        #print(f'hypers={hypers}')\n",
    "        \n",
    "        paramStack = []\n",
    "\n",
    "        \n",
    "        \n",
    "        # translate hyper-parameters to actual parameters as a matrix        \n",
    "        for z in range(nUnits):\n",
    "            if not self.noOutMode:\n",
    "                hypersSlice0 = hypers[self.hypersIdx:self.hypersIdx +nUnits*ncsCtx.nWaves*3]\n",
    "                #print('hyperSlice0:') # DBG\n",
    "                #print(hypersSlice0) # DBG\n",
    "                #print(len(hypersSlice0)) # DBG\n",
    "            self.hypersIdx += (nUnits*ncsCtx.nWaves*3)\n",
    "\n",
    "            if not self.noOutMode:\n",
    "                params0 = ncsCtx.calcSuperpositionByHyperparameters(matrixWidth, hypersSlice0)\n",
    "                paramStack.append(params0)\n",
    "\n",
    "        if self.noOutMode:\n",
    "            return None\n",
    "        \n",
    "        params1 = torch.stack(paramStack)\n",
    "        return params1\n",
    "\n",
    "    def readoutVectorFromHyperparameters(self, hypers, size, ncsCtx):\n",
    "        #print(f'DBG hypersIdx={self.hypersIdx}')\n",
    "        #print(f'hypers={hypers}')\n",
    "        \n",
    "        nParamsToExtract = size\n",
    "        if self.noOutMode:\n",
    "            params2 = None\n",
    "        else:\n",
    "            params2 = torch.tensor(hypers[self.hypersIdx:self.hypersIdx+nParamsToExtract])\n",
    "        self.hypersIdx += nParamsToExtract\n",
    "        return params2\n",
    "\n",
    "    # return hyperparameters for misc\n",
    "    def retNumberOfHyperparamsMisc(self):\n",
    "        return self.hypersIdx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ncsCtx = NeuralCompressedSearchCtx()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nWaves = 2\n",
    "#nUnits = 3 # number of neurons\n",
    "\n",
    "#matrixWidth = 10\n",
    "\n",
    "\n",
    "# how many attempts per training step?\n",
    "nAttemptsPerTrainingStep = 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ModelA(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelA, self).__init__()\n",
    "\n",
    "        xWidth = 4\n",
    "\n",
    "        \n",
    "        self.nUnitsHopfield = 3\n",
    "        self.widthHopfield = 9\n",
    "        self.hopfieldA = Hopfield(self.nUnitsHopfield, self.widthHopfield)\n",
    "\n",
    "\n",
    "        self.encoderForHopfieldAForR = LeakyReLUMlp(xWidth, 30, self.nUnitsHopfield*self.widthHopfield)\n",
    "        self.encoderForHopfieldAForY = LeakyReLUMlp(xWidth, 30, self.nUnitsHopfield*self.widthHopfield)\n",
    "\n",
    "\n",
    "        matrixWidth5 = xWidth + hopfield.n*hopfield.width\n",
    "        self.outLinearTransformWeights = torch.nn.Parameter(torch.rand(matrixWidth5, xWidth)*(1.0 / (matrixWidth5*xWidth))*0.1)\n",
    "\n",
    "        \n",
    "\n",
    "        #matrixWidth5 = hopfield.n*hopfield.width\n",
    "        matrixWidth5 = xWidth\n",
    "        nUnits5 = 3 # is the size of the output vector\n",
    "        self.logitHeadWeights = torch.nn.Parameter(torch.rand(matrixWidth5, nUnits5)*(1.0 / (matrixWidth5*nUnits5))*0.1)\n",
    "\n",
    "    def forward(self, x):#, R, Y):\n",
    "\n",
    "        #print(x) # DBG\n",
    "\n",
    "        R = self.encoderForHopfieldAForR.forward(x)\n",
    "        #print(R)\n",
    "        R = R.reshape(self.widthHopfield, self.nUnitsHopfield)\n",
    "\n",
    "        Y = self.encoderForHopfieldAForR.forward(x)\n",
    "        #print(R)\n",
    "        Y = Y.reshape(self.hopfieldA.n, self.hopfieldA.width)\n",
    "\n",
    "        hopfieldResA = self.hopfieldA.forward(R, Y)\n",
    "\n",
    "        hopfieldResAFlattened = torch.flatten(hopfieldResA)\n",
    "\n",
    "        # merge crossbar with output from NN of this layer\n",
    "        z0 = torch.cat([x, hopfieldResAFlattened])\n",
    "\n",
    "        # transform so that output of this module has dimensions like \"x\"\n",
    "        z0 = z0 @ self.outLinearTransformWeights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        logithead2Matrix = self.logitHeadWeights # transpose2(self.logitHeadWeights)\n",
    "\n",
    "        #print(hopfieldResAFlattened.size())\n",
    "        #print(logithead2Matrix.size())\n",
    "        \n",
    "        #z0 = hopfieldResAFlattened @ logithead2Matrix\n",
    "        z0 = z0 @ logithead2Matrix\n",
    "\n",
    "        return z0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "modelA = ModelA()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "readoutCtx = ReadoutCtx()\n",
    "readoutCtx.noOutMode = True # we don't want to have the actual parameters, we only want to compute the exact number of hyperparameters we need\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# readout parameters from hyperparameters for hopfield NN module\n",
    "# /param isWrite write from parameters to actual model?\n",
    "def readoutParamsOfModel(hypers, isWrite, model, ncsCtx):\n",
    "    for iParamName, iParam in model.named_parameters():\n",
    "        paramTensor = iParam.data # access actual tensor of parameter\n",
    "        #print(paramTensor.size()) # access actual tensor\n",
    "        \n",
    "        #print(iParamName + '   ' + str(iParam))\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        if iParamName == 'Wq' or iParamName.endswith('.Wq'):\n",
    "            # we treat Wq in a special way as a uncompressed matrix\n",
    "    \n",
    "            # TODO< do treat it in a actually special way! >\n",
    "\n",
    "            if len(paramTensor.size()) >= 2:\n",
    "                matrixWidth5 = paramTensor.size()[0]\n",
    "                nUnits5 = paramTensor.size()[1]\n",
    "            else:\n",
    "                matrixWidth5 = paramTensor.size()[0]\n",
    "                nUnits5 = 1\n",
    "                \n",
    "            z0 = readoutCtx.readoutMatrixFromCompressed(hypers, nUnits5, matrixWidth5, ncsCtx)\n",
    "\n",
    "\n",
    "            if isWrite:\n",
    "                #print('---')\n",
    "                #print(iParam.data.size())\n",
    "                z0 = torch.transpose(z0, 0, 1)\n",
    "                z0 = z0.reshape(iParam.data.size()) # make sure it has the right shape!\n",
    "                iParam.data = z0 #torch.transpose(z0, 0, 1)\n",
    "                #print(iParam.data.size())\n",
    "                pass\n",
    "            \n",
    "            pass\n",
    "        else:\n",
    "            # we treat this matrix as a compressed matrix\n",
    "    \n",
    "            matrixWidth5 = paramTensor.size()[0]\n",
    "            nUnits5 = 1\n",
    "            \n",
    "            if len(paramTensor.size()) >= 2:\n",
    "                matrixWidth5 = paramTensor.size()[0]\n",
    "                nUnits5 = paramTensor.size()[1]\n",
    "            else:\n",
    "                matrixWidth5 = paramTensor.size()[0]\n",
    "                nUnits5 = 1\n",
    "            \n",
    "            z0 = readoutCtx.readoutMatrixFromCompressed(hypers, nUnits5, matrixWidth5, ncsCtx)\n",
    "            \n",
    "\n",
    "            if isWrite:\n",
    "                #print('---')\n",
    "                #print(iParam.data.size())\n",
    "                z0 = torch.transpose(z0, 0, 1)\n",
    "                z0 = z0.reshape(iParam.data.size()) # make sure it has the right shape!\n",
    "                iParam.data = z0#torch.transpose(z0, 0, 1)\n",
    "                #print(iParam.data.size())\n",
    "                pass\n",
    "\n",
    "readoutParamsOfModel(None, False, modelA, ncsCtx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nHyperParams = readoutCtx.retNumberOfHyperparamsMisc()\n",
    "print(f'nHyperParams={nHyperParams}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# HACKY<   Y should be compute by MLP from input \"x\"   >\n",
    "Y = torch.rand(hopfield.n, hopfield.width)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# all hyperparameters of the best candidate\n",
    "#hypersBest = torch.tensor([0.1, 0.5, 0.52,     0.3, 0.05, 0.3,            0.888, 0.006, 0.52,     0.3, 0.05, 0.3,                  0.888, 0.006, 0.52,     0.3, 0.05, 0.3,              0.0001, 0.7, 0.333])\n",
    "\n",
    "hypersBest = torch.rand(nHyperParams)*0.01\n",
    "hypersBestLoss = 1.0e20\n",
    "\n",
    "for itOuter in range(1000):\n",
    "\n",
    "\n",
    "    hypersWithBestLossThisStep = hypersBest.clone()\n",
    "    bestLossThisStep = 1.0e20\n",
    "    \n",
    "    for itInner in range(nAttemptsPerTrainingStep):\n",
    "        hypers = hypersBest.clone()\n",
    "        \n",
    "        \n",
    "        #hyperparamVectorSize = (nWaves*3+1) * nUnits # +1  because for the hyperparameter for the bias\n",
    "        hypers = hypers + torch.normal(0.0, 0.01, size=hypers.size()) # normal distribution\n",
    "        \n",
    "        \n",
    "        #####################\n",
    "        # readout of parameters for NN from hyperparameters\n",
    "        \n",
    "        readoutCtx = ReadoutCtx()\n",
    "\n",
    "        # write parameters from hyperparameters to actual model\n",
    "        readoutParamsOfModel(hypers, True, modelA, ncsCtx)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        ####################\n",
    "        # INFERENCE\n",
    "\n",
    "        yTarget = torch.tensor([0.5, 0.9, 0.2])\n",
    "        \n",
    "        '''\n",
    "        # stimulus vector\n",
    "        x = torch.tensor([0.999, 0.9, -0.5, 0.0, 0.7,    0.5, 0.5, 0.1, 0.1, 0.0])\n",
    "        \n",
    "        z0 = x @ params1\n",
    "        z0 = z0 + params2\n",
    "        \n",
    "        #z0 = torch.nn.functional.softmax(z0, dim=0)  # nonlinearity\n",
    "        z0 = torch.nn.functional.leaky_relu(z0, negative_slope=0.05)  # nonlinearity\n",
    "        \n",
    "        #print(z0) # DBG\n",
    "        \n",
    "        y = z0\n",
    "        '''\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        R = transpose2( torch.tensor([[0.99, 0.01, 0.998,  0.11, 0.01, 0.00282,  0.00827, 0.0082629, 0.000986], [0.00882882, 0.992, 0.998,  0.11, 0.01, 0.00282,  0.00827, 0.0082629, 0.000986], [0.99, 0.01, 0.998,  0.999, 0.999, 0.00282,  0.00827, 0.0082629, 0.000986]]) )\n",
    "        #Y = torch.rand(hopfield.n, hopfield.width)\n",
    "        \n",
    "        '''\n",
    "        hopfieldOutA = hopfield.forward(R, Y)\n",
    "        \n",
    "\n",
    "        if False: # debug output from hopfield NN?\n",
    "            torch.set_printoptions(precision=10)\n",
    "            print(hopfieldOutA)\n",
    "            torch.set_printoptions(profile='default')\n",
    "        \n",
    "        #logithead2Matrix = torch.rand(hopfield.n*hopfield.width, 3) # 3 is the size of the output vector\n",
    "\n",
    "\n",
    "        \n",
    "        m1 = torch.flatten(hopfieldOutA)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        logithead2Matrix = transpose2(logitheadMatrix)\n",
    "\n",
    "        #print(m1.size())\n",
    "        #print(logithead2Matrix.size())\n",
    "        #print(logithead2Matrix)\n",
    "        \n",
    "        z0 = m1 @ logithead2Matrix\n",
    "\n",
    "        y = z0\n",
    "        \n",
    "        #print(y) # DBG\n",
    "\n",
    "        '''\n",
    "\n",
    "        x = torch.tensor([0.9, 0.8, 1.0, 0.5])\n",
    "\n",
    "        #y = modelA.forward(R, Y)\n",
    "        y = modelA.forward(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        loss = torch.nn.functional.mse_loss(y, yTarget)\n",
    "        \n",
    "        if False: # print loss?\n",
    "            print(f'loss={loss}')\n",
    "    \n",
    "        if loss < bestLossThisStep:\n",
    "            bestLossThisStep = loss\n",
    "            hypersWithBestLossThisStep = hypers\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # did we find a better solution?\n",
    "    if bestLossThisStep < hypersBestLoss:\n",
    "        hypersBest = hypersWithBestLossThisStep\n",
    "        hypersBestLoss = bestLossThisStep\n",
    "\n",
    "    if True and (itOuter % 50) == 0: # print loss?\n",
    "        print(f'hypersBestLoss={hypersBestLoss}')\n",
    "\n",
    "\n",
    "\n",
    "# HALFDONE< implement simple learning algorithm by generating a new candidate hyperparameter vector and then evaluate  the parameterized network on a (small) batch >\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
